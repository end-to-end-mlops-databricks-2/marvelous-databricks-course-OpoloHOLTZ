{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install loguru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # If in databricks\n",
    "# import sys\n",
    "\n",
    "# repo_path = \"/Workspace/Users/opolo.holtz@amaris.com/.bundle/marvelous-databricks-course-OpoloHOLTZ/dev/files/src\"\n",
    "# sys.path.append(repo_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "import hashlib\n",
    "\n",
    "import mlflow\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.service.serving import (\n",
    "    EndpointCoreConfigInput,\n",
    "    ServedEntityInput,\n",
    ")\n",
    "from mlflow.models import infer_signature\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from defaultccc.config import ProjectConfig, Tags\n",
    "from defaultccc.models.model_basic import BasicModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default profile:\n",
    "mlflow.set_tracking_uri(\"databricks://opoloholtz\")\n",
    "mlflow.set_registry_uri(\"databricks-uc://opoloholtz\")\n",
    "\n",
    "config = ProjectConfig.from_yaml(config_path=\"../project_config.yml\")\n",
    "catalog_name = config.catalog_name\n",
    "schema_name = config.schema_name\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "tags = Tags(**{\"git_sha\": \"abcd12345\", \"branch\": \"week3\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-03-07 17:30:34.115\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdefaultccc.models.model_basic\u001b[0m:\u001b[36mload_data\u001b[0m:\u001b[36m40\u001b[0m - \u001b[1mLoading data from maven_training_databricks.default_ccc tables train and test...\u001b[0m\n",
      "\u001b[32m2025-03-07 17:30:37.141\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdefaultccc.models.model_basic\u001b[0m:\u001b[36mload_data\u001b[0m:\u001b[36m53\u001b[0m - \u001b[1mData succesfully loaded.\u001b[0m\n",
      "\u001b[32m2025-03-07 17:30:37.142\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdefaultccc.models.model_basic\u001b[0m:\u001b[36mprepare_features\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1mStarting the preprocesing with a pipeline...\u001b[0m\n",
      "\u001b[32m2025-03-07 17:30:37.143\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdefaultccc.models.model_basic\u001b[0m:\u001b[36mprepare_features\u001b[0m:\u001b[36m77\u001b[0m - \u001b[1mPreprocessing data pipeline succeded\u001b[0m\n",
      "\u001b[32m2025-03-07 17:30:37.143\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdefaultccc.models.model_basic\u001b[0m:\u001b[36mtrain_model\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mStarting to train...\u001b[0m\n",
      "\u001b[32m2025-03-07 17:30:37.698\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdefaultccc.models.model_basic\u001b[0m:\u001b[36mtrain_model\u001b[0m:\u001b[36m85\u001b[0m - \u001b[1mModel training completed.\u001b[0m\n",
      "\u001b[32m2025-03-07 17:30:48.573\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdefaultccc.models.model_basic\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m91\u001b[0m - \u001b[1mEvaluating the model...\u001b[0m\n",
      "\u001b[32m2025-03-07 17:30:48.650\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdefaultccc.models.model_basic\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mModel Evaluation:\n",
      "Accuracy: 0.7735\n",
      "Precision: 0.4854060913705584\n",
      "Recall: 0.5826351865955827\n",
      "F1 Score: 0.5295950155763239\n",
      "ROC AUC: 0.7675025523920825\u001b[0m\n",
      "\u001b[32m2025-03-07 17:30:48.651\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdefaultccc.models.model_basic\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m107\u001b[0m - \u001b[1mConfusion Matrix:\n",
      "[[3876  811]\n",
      " [ 548  765]]\u001b[0m\n",
      "/home/opoloholtz_amaris/Repos/marvelous-databricks-course-OpoloHOLTZ/.venv/lib/python3.11/site-packages/mlflow/types/utils.py:407: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n",
      "2025/03/07 17:30:54 WARNING mlflow.data.spark_dataset: Encountered an unexpected exception while computing Spark dataset profile. Exception: [NOT_IMPLEMENTED] rdd is not implemented.\n",
      "\u001b[32m2025-03-07 17:30:58.202\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdefaultccc.models.model_basic\u001b[0m:\u001b[36mlog_model\u001b[0m:\u001b[36m148\u001b[0m - \u001b[1mModel logged successfully with Run ID: db9f76a3dc76440e98e9d230bf7a487c\u001b[0m\n",
      "2025/03/07 17:30:58 INFO mlflow.tracking._tracking_service.client: üèÉ View run thundering-pug-136 at: https://adb-4498504268974234.14.azuredatabricks.net/ml/experiments/3819452030974426/runs/db9f76a3dc76440e98e9d230bf7a487c.\n",
      "2025/03/07 17:30:58 INFO mlflow.tracking._tracking_service.client: üß™ View experiment at: https://adb-4498504268974234.14.azuredatabricks.net/ml/experiments/3819452030974426.\n",
      "\u001b[32m2025-03-07 17:30:58.705\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdefaultccc.models.model_basic\u001b[0m:\u001b[36mregister_model\u001b[0m:\u001b[36m154\u001b[0m - \u001b[1mRegistering the model maven_training_databricks.default_ccc.default_ccc_model_basic_A in the UC...\u001b[0m\n",
      "Registered model 'maven_training_databricks.default_ccc.default_ccc_model_basic_A' already exists. Creating a new version of this model...\n",
      "Created version '2' of model 'maven_training_databricks.default_ccc.default_ccc_model_basic_a'.\n",
      "\u001b[32m2025-03-07 17:31:02.903\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdefaultccc.models.model_basic\u001b[0m:\u001b[36mregister_model\u001b[0m:\u001b[36m162\u001b[0m - \u001b[1mModel registered as version 2.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Train & register model A with the config path\n",
    "basic_model_a = BasicModel(config=config, tags=tags, spark=spark)\n",
    "basic_model_a.paramaters = config.parameters_a\n",
    "basic_model_a.model_name = f\"{catalog_name}.{schema_name}.default_ccc_model_basic_A\"\n",
    "basic_model_a.load_data()\n",
    "basic_model_a.prepare_features()\n",
    "basic_model_a.train_model()\n",
    "basic_model_a.log_model()\n",
    "basic_model_a.register_model()\n",
    "model_A = mlflow.sklearn.load_model(f\"models:/{basic_model_a.model_name}@latest-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-03-07 17:31:13.122\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdefaultccc.models.model_basic\u001b[0m:\u001b[36mload_data\u001b[0m:\u001b[36m40\u001b[0m - \u001b[1mLoading data from maven_training_databricks.default_ccc tables train and test...\u001b[0m\n",
      "\u001b[32m2025-03-07 17:31:15.285\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdefaultccc.models.model_basic\u001b[0m:\u001b[36mload_data\u001b[0m:\u001b[36m53\u001b[0m - \u001b[1mData succesfully loaded.\u001b[0m\n",
      "\u001b[32m2025-03-07 17:31:15.286\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdefaultccc.models.model_basic\u001b[0m:\u001b[36mprepare_features\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1mStarting the preprocesing with a pipeline...\u001b[0m\n",
      "\u001b[32m2025-03-07 17:31:15.286\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdefaultccc.models.model_basic\u001b[0m:\u001b[36mprepare_features\u001b[0m:\u001b[36m77\u001b[0m - \u001b[1mPreprocessing data pipeline succeded\u001b[0m\n",
      "\u001b[32m2025-03-07 17:31:15.287\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdefaultccc.models.model_basic\u001b[0m:\u001b[36mtrain_model\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mStarting to train...\u001b[0m\n",
      "\u001b[32m2025-03-07 17:31:15.802\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdefaultccc.models.model_basic\u001b[0m:\u001b[36mtrain_model\u001b[0m:\u001b[36m85\u001b[0m - \u001b[1mModel training completed.\u001b[0m\n",
      "\u001b[32m2025-03-07 17:31:16.868\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdefaultccc.models.model_basic\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m91\u001b[0m - \u001b[1mEvaluating the model...\u001b[0m\n",
      "\u001b[32m2025-03-07 17:31:16.960\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdefaultccc.models.model_basic\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mModel Evaluation:\n",
      "Accuracy: 0.7735\n",
      "Precision: 0.4854060913705584\n",
      "Recall: 0.5826351865955827\n",
      "F1 Score: 0.5295950155763239\n",
      "ROC AUC: 0.7675025523920825\u001b[0m\n",
      "\u001b[32m2025-03-07 17:31:16.962\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdefaultccc.models.model_basic\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m107\u001b[0m - \u001b[1mConfusion Matrix:\n",
      "[[3876  811]\n",
      " [ 548  765]]\u001b[0m\n",
      "/home/opoloholtz_amaris/Repos/marvelous-databricks-course-OpoloHOLTZ/.venv/lib/python3.11/site-packages/mlflow/types/utils.py:407: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n",
      "2025/03/07 17:31:21 WARNING mlflow.data.spark_dataset: Encountered an unexpected exception while computing Spark dataset profile. Exception: [NOT_IMPLEMENTED] rdd is not implemented.\n",
      "\u001b[32m2025-03-07 17:31:22.306\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdefaultccc.models.model_basic\u001b[0m:\u001b[36mlog_model\u001b[0m:\u001b[36m148\u001b[0m - \u001b[1mModel logged successfully with Run ID: f88b5d9a19884a60b9b8062b96c72821\u001b[0m\n",
      "2025/03/07 17:31:22 INFO mlflow.tracking._tracking_service.client: üèÉ View run enthused-jay-623 at: https://adb-4498504268974234.14.azuredatabricks.net/ml/experiments/3819452030974426/runs/f88b5d9a19884a60b9b8062b96c72821.\n",
      "2025/03/07 17:31:22 INFO mlflow.tracking._tracking_service.client: üß™ View experiment at: https://adb-4498504268974234.14.azuredatabricks.net/ml/experiments/3819452030974426.\n",
      "\u001b[32m2025-03-07 17:31:22.764\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdefaultccc.models.model_basic\u001b[0m:\u001b[36mregister_model\u001b[0m:\u001b[36m154\u001b[0m - \u001b[1mRegistering the model maven_training_databricks.default_ccc.default_ccc_model_basic_B in the UC...\u001b[0m\n",
      "Registered model 'maven_training_databricks.default_ccc.default_ccc_model_basic_B' already exists. Creating a new version of this model...\n",
      "Created version '2' of model 'maven_training_databricks.default_ccc.default_ccc_model_basic_b'.\n",
      "\u001b[32m2025-03-07 17:31:26.367\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdefaultccc.models.model_basic\u001b[0m:\u001b[36mregister_model\u001b[0m:\u001b[36m162\u001b[0m - \u001b[1mModel registered as version 2.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Train & register model B with the config path\n",
    "basic_model_b = BasicModel(config=config, tags=tags, spark=spark)\n",
    "basic_model_b.paramaters = config.parameters_b\n",
    "basic_model_b.model_name = f\"{catalog_name}.{schema_name}.default_ccc_model_basic_B\"\n",
    "basic_model_b.load_data()\n",
    "basic_model_b.prepare_features()\n",
    "basic_model_b.train_model()\n",
    "basic_model_b.log_model()\n",
    "basic_model_b.register_model()\n",
    "model_B = mlflow.sklearn.load_model(f\"models:/{basic_model_b.model_name}@latest-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HousePriceModelWrapper(mlflow.pyfunc.PythonModel):\n",
    "    def __init__(self, models):\n",
    "        self.models = models\n",
    "        self.model_a = models[0]\n",
    "        self.model_b = models[1]\n",
    "\n",
    "    def predict(self, context, model_input):\n",
    "        defaultccc_id = str(model_input[\"ID\"].values[0])\n",
    "        hashed_id = hashlib.md5(defaultccc_id.encode(encoding=\"UTF-8\")).hexdigest()\n",
    "        # convert a hexadecimal (base-16) string into an integer\n",
    "        if int(hashed_id, 16) % 2:\n",
    "            predictions = self.model_a.predict(model_input.drop([\"ID\"], axis=1))\n",
    "            return {\"Prediction\": predictions[0], \"model\": \"Model A\"}\n",
    "        else:\n",
    "            predictions = self.model_b.predict(model_input.drop([\"ID\"], axis=1))\n",
    "            return {\"Prediction\": predictions[0], \"model\": \"Model B\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_spark = spark.table(f\"{catalog_name}.{schema_name}.train_set\")\n",
    "train_set = train_set_spark.toPandas()\n",
    "test_set = spark.table(f\"{catalog_name}.{schema_name}.test_set\").toPandas()\n",
    "X_train = train_set[config.num_features + config.cat_features + [\"ID\"]]\n",
    "X_test = test_set[config.num_features + config.cat_features + [\"ID\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Prediction: {'Prediction': 0, 'model': 'Model B'}\n"
     ]
    }
   ],
   "source": [
    "models = [model_A, model_B]\n",
    "wrapped_model = HousePriceModelWrapper(models)  # we pass the loaded models to the wrapper\n",
    "example_input = X_test.iloc[0:1]  # Select the first row for prediction as example\n",
    "example_prediction = wrapped_model.predict(context=None, model_input=example_input)\n",
    "print(\"Example Prediction:\", example_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/03/07 17:33:35 WARNING mlflow.data.spark_dataset: Encountered an unexpected exception while computing Spark dataset profile. Exception: [NOT_IMPLEMENTED] rdd is not implemented.\n",
      "2025/03/07 17:33:39 INFO mlflow.tracking._tracking_service.client: üèÉ View run amazing-hen-75 at: https://adb-4498504268974234.14.azuredatabricks.net/ml/experiments/3772110128274668/runs/07e5526649db4c7a81d048576a1585b2.\n",
      "2025/03/07 17:33:39 INFO mlflow.tracking._tracking_service.client: üß™ View experiment at: https://adb-4498504268974234.14.azuredatabricks.net/ml/experiments/3772110128274668.\n",
      "Registered model 'maven_training_databricks.default_ccc.default_ccc_model_pyfunc_ab_test' already exists. Creating a new version of this model...\n",
      "Created version '3' of model 'maven_training_databricks.default_ccc.default_ccc_model_pyfunc_ab_test'.\n"
     ]
    }
   ],
   "source": [
    "mlflow.set_experiment(experiment_name=\"/Shared/default-ccc-ab-testing\")\n",
    "model_name = f\"{catalog_name}.{schema_name}.default_ccc_model_pyfunc_ab_test\"\n",
    "artifact_name = \"pyfunc-default-ccc-model-ab\"\n",
    "\n",
    "with mlflow.start_run() as run:\n",
    "    run_id = run.info.run_id\n",
    "    signature = infer_signature(model_input=X_train, model_output={\"Prediction\": 1, \"model\": \"Model B\"})\n",
    "    dataset = mlflow.data.from_spark(train_set_spark, table_name=f\"{catalog_name}.{schema_name}.train_set\", version=\"0\")\n",
    "    mlflow.log_input(dataset, context=\"training\")\n",
    "    mlflow.pyfunc.log_model(python_model=wrapped_model, artifact_path=artifact_name, signature=signature)\n",
    "model_version = mlflow.register_model(model_uri=f\"runs:/{run_id}/{artifact_name}\", name=model_name, tags=tags.dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<databricks.sdk.service._internal.Wait at 0x7f265a41b510>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workspace = WorkspaceClient()\n",
    "served_entities = [\n",
    "    ServedEntityInput(\n",
    "        entity_name=model_name,\n",
    "        scale_to_zero_enabled=True,\n",
    "        workload_size=\"Small\",\n",
    "        entity_version=model_version.version,\n",
    "    )\n",
    "]\n",
    "\n",
    "workspace.serving_endpoints.create(\n",
    "    name=f\"{artifact_name}-serving\",\n",
    "    config=EndpointCoreConfigInput(\n",
    "        served_entities=served_entities,\n",
    "    ),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
