{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install loguru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "\n",
    "# repo_path = \"/Workspace/Users/opolo.holtz@amaris.com/.bundle/marvelous-databricks-course-OpoloHOLTZ/dev/files/src\"\n",
    "# sys.path.append(repo_path)\n",
    "\n",
    "# # VÃ©rifier que le chemin est bien pris en compte\n",
    "# print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from pyspark.dbutils import DBUtils\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from defaultccc.config import ProjectConfig\n",
    "from defaultccc.serving.model_serving import ModelServing\n",
    "\n",
    "# spark session\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "dbutils = DBUtils(spark)\n",
    "\n",
    "# get environment variables\n",
    "os.environ[\"DBR_TOKEN\"] = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()\n",
    "os.environ[\"DBR_HOST\"] = spark.conf.get(\"spark.databricks.workspaceUrl\")\n",
    "\n",
    "# Load project config\n",
    "config = ProjectConfig.from_yaml(config_path=\"../project_config.yml\")\n",
    "catalog_name = config.catalog_name\n",
    "schema_name = config.schema_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize feature store manager\n",
    "model_serving = ModelServing(\n",
    "    model_name=f\"{catalog_name}.{schema_name}.default_ccc_model_basic\", endpoint_name=\"default_ccc-model-serving\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy the model serving endpoint\n",
    "model_serving.deploy_or_update_serving_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample request body\n",
    "required_columns = config.num_features + config.cat_features\n",
    "\n",
    "# Sample 1000 records from the training set\n",
    "test_set = spark.table(f\"{config.catalog_name}.{config.schema_name}.test_set\").toPandas()\n",
    "\n",
    "# Sample 100 records from the training set\n",
    "sampled_records = test_set[required_columns].sample(n=100, replace=True).to_dict(orient=\"records\")\n",
    "dataframe_records = [[record] for record in sampled_records]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_records[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_serving.call_endpoint(dataframe_records[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_serving.evaluate_serving_model(\n",
    "    test_set[required_columns + [config.target]].sample(n=100, replace=True), config.target\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
